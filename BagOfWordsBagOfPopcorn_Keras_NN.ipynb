{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and Splitting data\n",
      "Building Tokenizer and NN Model, if they dont exist\n",
      "Building Tokenizer and NN Model\n",
      "Found 74065 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:145: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/Users/ram/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:150: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"same\", filters=32, kernel_size=3)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               39900     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 203,105\n",
      "Trainable params: 203,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model Fit\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 333s - loss: 0.5081 - acc: 0.7296   \n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 303s - loss: 0.3137 - acc: 0.8706   \n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 287s - loss: 0.2618 - acc: 0.8954   \n",
      "Saving Model\n",
      "Loading Classifier and Tokenizer\n",
      "Converting reviews to ascii, tokenize and pad reviews \n",
      "Making predictions...\n",
      "Printing AUC Scores...\n",
      "AUC: 0.94164608314\n"
     ]
    }
   ],
   "source": [
    "# Generic Libraries\n",
    "import os.path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Text Processing\n",
    "from bs4 import BeautifulSoup  # HTML to text\n",
    "from nltk.corpus import stopwords  # String cleaning\n",
    "import nltk.data  # To load sentence tokenizer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# joblib is used to store/load passage tokenizers and/or models\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Dropout, Convolution1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend\n",
    "from keras.models import load_model\n",
    "\n",
    "# Folder where models and tokenizers are stored\n",
    "HOME_DIR = '/Users/ram/Desktop/BagPopcorn'\n",
    "os.chdir(HOME_DIR)\n",
    "\n",
    "# Random Seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Model and Tokenizer\n",
    "KERAS_NN_MODEL = \"keras_nn_model.h5\"\n",
    "KERAS_TOKENIZER = \"keras_tokenizer.pkl\"\n",
    "MAX_REVIEW_LENGTH_FOR_KERAS_RNN = 500\n",
    "\n",
    "# Text Preprocessing\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    # print (\"Length of the Review-\",len(raw_review))\n",
    "    review_text = BeautifulSoup(raw_review,\"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Tokenization : Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    words_M = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. WordNetLemmatizer should better be used with POS tagging\n",
    "    wl = nltk.WordNetLemmatizer()\n",
    "    words_L = [wl.lemmatize(word) for word in words_M]\n",
    "    #\n",
    "    # 7. PorterStemmer(), LancasterStemmer(), SnowballStemmer()\n",
    "    # stemming can often create non-existent words, whereas lemmas are actual words.\n",
    "    ps = nltk.PorterStemmer()\n",
    "    words_S = [ps.stem(word) for word in words_L]\n",
    "    # 8. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( words_M ))     \n",
    "\n",
    "\n",
    "# Train and save Keras tokenizer and Keras CNN model\n",
    "def keras_tokenizer_cnn_model(x_train, y_train, x_test, use_cnn=True):\n",
    "    \n",
    "    #---------------------------------- Tokenization ----------------------------------#\n",
    "    # 1. The Tokenizer class in Keras has various methods which help to prepare text so it can be used in neural network models.\n",
    "    # 2. The top-n words num_words will not truncate the words found in the input but it will truncate the usage. \n",
    "    # 3. Assigns each word in the reviews an ID corresponding to its frequency rank.\n",
    "    # (Top 5000 most frequent words - This is the size of the vocabulary in the text data.)\n",
    "    \n",
    "    num_most_freq_words_to_include = 5000\n",
    "    tokenizer = Tokenizer(num_words=num_most_freq_words_to_include)\n",
    "\n",
    "    # Convert unicode strings into ascii to avoid tokenization errors\n",
    "    train_reviews_list = [s.encode('ascii') for s in x_train.tolist()]\n",
    "    \n",
    "    # The Tokenizer stores everything in the word_index during fit_on_texts. \n",
    "    # Then, when calling the texts_to_sequences method, only the top num_words are considered.\n",
    "    tokenizer.fit_on_texts(all_reviews_list)\n",
    "    #pprint(tokenizer.word_index)\n",
    "    print('Found {} unique tokens.'.format(len(tokenizer.word_index)))\n",
    "    \n",
    "    # The train_reviews_list is converted into a list of numbers which are the high frequency ranked \n",
    "    # from tokenizer.word_index. Ex: HOW ARE U => [10 11 98] where 10,11 and 98 are from tokenizer.word_index\n",
    "    train_reviews_tokenized = tokenizer.texts_to_sequences(train_reviews_list)\n",
    "    \n",
    "    #print type(train_reviews_tokenized)\n",
    "    #print type(tokenizer.word_counts)\n",
    "    # Number of times a particular word appeared in the sentence\n",
    "    #pprint(tokenizer.word_counts)\n",
    "\n",
    "    # Truncate and pad input sequences, so that we only cover up to the first 500 tokens per review\n",
    "    # This ensures all reviews have a representation of the same size, which is needed for \n",
    "    # the Keras NN to process them.\n",
    "    x_train = sequence.pad_sequences(train_reviews_tokenized, maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "    \n",
    "    #---------------------------------- CNN ----------------------------------#\n",
    "   \n",
    "    # Indicate it's a sequential type of model - linear stack of layers\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding dropout value\n",
    "    initial_dropout = 0.2  \n",
    "\n",
    "    # Embedding layer\n",
    "    # Keras offers an Embedding layer that can be used for neural networks on text data.It requires that\n",
    "    # the input data be integer encoded, so that each word is represented by a unique integer. This data \n",
    "    # preparation step can be performed using the Tokenizer API also provided with Keras. The Embedding layer \n",
    "    # is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "    \n",
    "    # This is the size of the vector space in which words will be embedded. \n",
    "    # It defines the size of the output vectors from this layer for each word. \n",
    "    # For example, it could be 32 or 100 or even larger. \n",
    "    embedding_vector_length = 32  \n",
    "    model.add(Embedding(num_most_freq_words_to_include, \n",
    "                        embedding_vector_length,\n",
    "                        input_length=MAX_REVIEW_LENGTH_FOR_KERAS_RNN, \n",
    "                        dropout=initial_dropout))   \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # CNN and MaxPool layer\n",
    "    if use_cnn:\n",
    "        model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # GRU layer\n",
    "    model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Dense layer to get final probability prediction\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #---------------------------------- CNN ----------------------------------#\n",
    "    \n",
    "    print (\"Model Summary\")\n",
    "    print(model.summary())\n",
    "\n",
    "    print (\"Model Fit\")\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=64)  # , validation_split=0.2)\n",
    "\n",
    "    print (\"Saving Model\")\n",
    "    model.save(KERAS_NN_MODEL)\n",
    "    _ = joblib.dump(tokenizer, KERAS_TOKENIZER, compress=9)\n",
    "\n",
    "def get_train_test_data(review_to_words):\n",
    "    \n",
    "    df = pd.read_csv('labeledTrainData.tsv', header=0, quotechar='\"', sep='\\t')\n",
    "    #df = df.head(50)\n",
    "    \n",
    "    # Shuffle data frame rows\n",
    "    df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    x = df[\"review\"].map(review_to_words)\n",
    "    y = df[\"sentiment\"]\n",
    "    \n",
    "    # Split 80/20\n",
    "    test_start_index = int(df.shape[0] * .8)\n",
    "    x_train = x[0:test_start_index]\n",
    "    y_train = y[0:test_start_index]\n",
    "    x_test = x[test_start_index:]\n",
    "    y_test = y[test_start_index:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Keras NN -> Embeddings Layer + GRU Layer + Dense Layer only\n",
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    print (\"Preprocessing and Splitting data\")\n",
    "    x_train, y_train, x_test, y_test = get_train_test_data(review_to_words)\n",
    "    \n",
    "    print (\"Building Tokenizer and NN Model, if they dont exist\")\n",
    "    if not os.path.isfile(KERAS_NN_MODEL) or not os.path.isfile(KERAS_TOKENIZER):\n",
    "        print(\"Building Tokenizer and NN Model\")\n",
    "        keras_tokenizer_cnn_model(x_train, y_train, x_test)\n",
    "    else:\n",
    "        print(\"Loading existing Keras NN Model\")\n",
    "    \n",
    "    print (\"Loading Classifier and Tokenizer\")    \n",
    "    classifier = load_model(KERAS_NN_MODEL)\n",
    "    tokenizer  = joblib.load(KERAS_TOKENIZER)\n",
    "\n",
    "    print (\"Converting reviews to ascii, tokenize and pad reviews \")\n",
    "    test_reviews_list = [s.encode('ascii') for s in x_test.tolist()]\n",
    "    test_reviews_tokenized = tokenizer.texts_to_sequences(test_reviews_list)\n",
    "    test_reviews_tokenized_padded = sequence.pad_sequences(test_reviews_tokenized,maxlen=MAX_REVIEW_LENGTH_FOR_KERAS_RNN)\n",
    "    \n",
    "    print(\"Making predictions...\")\n",
    "    y_predict = classifier.predict(test_reviews_tokenized_padded)\n",
    "    \n",
    "    print(\"Printing AUC Scores...\")\n",
    "    print(\"AUC: {}\".format(roc_auc_score(y_test, y_predict, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
